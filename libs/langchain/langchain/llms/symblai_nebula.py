import logging
from typing import Any, Dict, List, Mapping, Optional

import requests
from pydantic import Extra, root_validator

from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM
from langchain.llms.utils import enforce_stop_tokens
from langchain.utils import get_from_dict_or_env

DEFAULT_SYMBLAI_NEBULA_SERVICE_URL = "https://api-nebula.symbl.ai"
DEFAULT_SYMBLAI_NEBULA_SERVICE_PATH = "/v1/model/generate"

logger = logging.getLogger(__name__)


class Nebula(LLM):
    """Nebula Service models.

    To use, you should have the environment variable ``SYMBLAI_NEBULA_SERVICE_URL``,
    ``SYMBLAI_NEBULA_SERVICE_PATH`` and ``SYMBLAI_NEBULA_SERVICE_TOKEN`` set with your Nebula
    Service, or pass it as a named parameter to the constructor.

    Example:
        .. code-block:: python

            from langchain.llms import Nebula

            nebula = Nebula(
                nebula_service_url="SERVICE_URL",
                nebula_service_path="SERVICE_ROUTE",
                nebula_service_token="SERVICE_TOKEN",
            )

            # Use Ray for distributed processing
            import ray

            prompt_list=[]

            @ray.remote
            def send_query(llm, prompt):
                resp = llm(prompt)
                return resp

            futures = [send_query.remote(nebula, prompt) for prompt in prompt_list]
            results = ray.get(futures)
    """  # noqa: E501

    """Key/value arguments to pass to the model. Reserved for future use"""
    model_kwargs: Optional[dict] = None

    """Optional"""
    nebula_service_url: Optional[str] = None
    nebula_service_path: Optional[str] = None
    nebula_service_token: Optional[str] = None
    conversation: str = ""
    return_scores: Optional[str] = "false"
    max_new_tokens: Optional[int] = 2048
    top_k: Optional[float] = 2
    penalty_alpha: Optional[float] = 0.1

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that api key and python package exists in environment."""
        nebula_service_url = get_from_dict_or_env(
            values, "nebula_service_url", "SYMBLAI_NEBULA_SERVICE_URL"
        )
        nebula_service_path = get_from_dict_or_env(
            values, "nebula_service_path", "SYMBLAI_NEBULA_SERVICE_PATH"
        )
        nebula_service_token = get_from_dict_or_env(
            values, "nebula_service_token", "SYMBLAI_NEBULA_SERVICE_TOKEN"
        )

        if len(nebula_service_url) == 0:
            nebula_service_url = DEFAULT_SYMBLAI_NEBULA_SERVICE_URL
        if len(nebula_service_path) == 0:
            nebula_service_path = DEFAULT_SYMBLAI_NEBULA_SERVICE_PATH

        if nebula_service_url.endswith("/"):
            nebula_service_url = nebula_service_url[:-1]
        if not nebula_service_path.startswith("/"):
            nebula_service_path = "/" + nebula_service_path

        """ TODO: Future login"""
        """
        try:
            nebula_service_endpoint = f"{nebula_service_url}{nebula_service_path}"
            headers = {
                "Content-Type": "application/json",
                "ApiKey": f"Bearer {nebula_service_token}",
                }
            requests.get(nebula_service_endpoint, headers=headers)
        except requests.exceptions.RequestException as e:
            raise ValueError(e)
        """

        values["nebula_service_url"] = nebula_service_url
        values["nebula_service_path"] = nebula_service_path
        values["nebula_service_token"] = nebula_service_token

        return values

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        _model_kwargs = self.model_kwargs or {}
        return {
            "nebula_service_url": self.nebula_service_url,
            "nebula_service_path": self.nebula_service_path,
            **{"model_kwargs": _model_kwargs},
            "conversation": self.conversation,
        }

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "nebula"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call out to Nebula Service endpoint.
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
        Returns:
            The string generated by the model.
        Example:
            .. code-block:: python
                response = nebula("Tell me a joke.")
        """

        _model_kwargs = self.model_kwargs or {}

        nebula_service_endpoint = f"{self.nebula_service_url}{self.nebula_service_path}"

        headers = {
            "Content-Type": "application/json",
            "ApiKey": f"Bearer {self.nebula_service_token}",
        }

        body = {
            "prompt": {
                "instruction": prompt,
                "conversation": {"text": f"{self.conversation}"},
            },
            "return_scores": self.return_scores,
            "max_new_tokens": self.max_new_tokens,
            "top_k": self.top_k,
            "penalty_alpha": self.penalty_alpha,
        }

        if len(self.conversation) == 0:
            raise ValueError("Error conversation is empty.")

        logger.debug(f"NEBULA _model_kwargs: {_model_kwargs}")
        logger.debug(f"NEBULA body: {body}")
        logger.debug(f"NEBULA kwargs: {kwargs}")
        logger.debug(f"NEBULA conversation: {self.conversation}")

        # call API
        try:
            response = requests.post(
                nebula_service_endpoint, headers=headers, json=body
            )
        except requests.exceptions.RequestException as e:
            raise ValueError(f"Error raised by inference endpoint: {e}")

        logger.debug(f"NEBULA response: {response}")

        if response.status_code != 200:
            raise ValueError(
                f"Error returned by service, status code {response.status_code}"
            )

        """ get the result """
        text = response.text

        """ enforce stop """
        if stop is not None:
            # This is required since the stop tokens
            # are not enforced by the model parameters
            text = enforce_stop_tokens(text, stop)

        return text
